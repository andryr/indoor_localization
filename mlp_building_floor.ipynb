{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "78e0bdf2-b6ac-49d1-b32b-928b81c0ba6a",
    "_uuid": "8fc2d8b8-8e33-4993-83d5-c9e6573d8cc7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "whole_train_df = pd.read_csv('uji_wifi/UJIndoorLoc/trainingData.csv')\n",
    "test_df = pd.read_csv('uji_wifi/UJIndoorLoc/ValidationData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the training data into training and validation sets using the USERID field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "9ca11851-fc46-49a1-94bb-3e878012c27e",
    "_uuid": "b59cee6b-383e-4563-91d5-78929610fd26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15647, 529) (4290, 529)\n"
     ]
    }
   ],
   "source": [
    "train_mask = whole_train_df['USERID'] <= 13\n",
    "train_df = whole_train_df[train_mask]\n",
    "val_df = whole_train_df[~train_mask]\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the data into NumPy arrays. Lack of AP measurement is represented by RSSI value of -110dBm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "1536fea0-aafd-4730-b7f6-522a8fefc326",
    "_uuid": "45434fc4-23d2-407b-9792-0409490a660e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = train_df.iloc[:,0:520].to_numpy()\n",
    "train_building = train_df[\"BUILDINGID\"].to_numpy(dtype=np.int64)\n",
    "train_floor = train_df[\"FLOOR\"].to_numpy(dtype=np.int64)\n",
    "train_long = train_df[\"LONGITUDE\"].to_numpy()\n",
    "train_lat = train_df[\"LATITUDE\"].to_numpy()\n",
    "\n",
    "val_X = val_df.iloc[:,0:520].to_numpy()\n",
    "val_building = val_df[\"BUILDINGID\"].to_numpy(dtype=np.int64)\n",
    "val_floor = val_df[\"FLOOR\"].to_numpy(dtype=np.int64)\n",
    "val_long = val_df[\"LONGITUDE\"].to_numpy()\n",
    "val_lat = val_df[\"LATITUDE\"].to_numpy()\n",
    "\n",
    "test_X = test_df.iloc[:,0:520].to_numpy()\n",
    "test_building = test_df[\"BUILDINGID\"].to_numpy(dtype=np.int64)\n",
    "test_floor = test_df[\"FLOOR\"].to_numpy(dtype=np.int64)\n",
    "test_long = test_df[\"LONGITUDE\"].to_numpy()\n",
    "test_lat = test_df[\"LATITUDE\"].to_numpy()\n",
    "\n",
    "\n",
    "train_X[train_X == 100] = -110\n",
    "val_X[val_X == 100] = -110\n",
    "test_X[test_X == 100] = -110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions that we will use for to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "7768ba83-2224-48e9-863c-7cc914ad06a1",
    "_uuid": "7e1b3991-2954-4536-b4fe-7a8dfc1ac804"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def accuracy(pred_Y, true_Y):\n",
    "    return np.sum(pred_Y == true_Y)/len(true_Y)\n",
    "\n",
    "def distance_rmse(pred_long, pred_lat, true_long, true_lat):\n",
    "    sq_dist = (pred_long - true_long)**2 + (pred_lat - true_lat)**2\n",
    "    return np.sqrt(np.sum(sq_dist)/len(sq_dist))\n",
    "\n",
    "def dist_mean_error(pred_long, pred_lat, true_long, true_lat):\n",
    "    dist = np.sqrt((pred_long - true_long)**2 + (pred_lat - true_lat)**2)\n",
    "    return np.sum(dist)/len(pred_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple MLP with only one hidden layer and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "1d1b4466-976e-4b28-a15d-5c5770871e17",
    "_uuid": "22fa7d90-a730-4441-8c74-7d10723b02c8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLPClf(nn.Module):\n",
    "    def __init__(self, num_outputs=1):\n",
    "        super(MLPClf, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(520, 256) \n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.final = nn.Linear(256, num_outputs)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.linear1(x)))\n",
    "    \n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "da74c14d-e48e-4ab3-89c2-830ad16ead6d",
    "_uuid": "1b7a128b-0e22-4ace-b3e4-8d3f48b2f2c9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, patience=10, transform=None):\n",
    "    data_loaders = {'train': train_loader, 'val': val_loader}\n",
    "    data_lengths = {'train': len(train_loader), 'val': len(val_loader)}\n",
    "    best_val_loss = None\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        epoch_start = time.time()\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(data_loaders[phase]):   \n",
    "                inputs, targets = data\n",
    "                inputs, targets = inputs.cuda(),targets.cuda()\n",
    "                if phase == 'train' and transform is not None:\n",
    "                    inputs = transform(inputs)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs).squeeze(-1) \n",
    "                loss = criterion(outputs,targets)  \n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward() \n",
    "                    optimizer.step() \n",
    "\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            epoch_loss = running_loss/data_lengths[phase]\n",
    "            if phase == 'train':\n",
    "                print(f'Training loss : {epoch_loss}')\n",
    "                print(f'Learning rate : {optimizer.param_groups[0][\"lr\"]}')\n",
    "            else:\n",
    "                scheduler.step(epoch_loss)\n",
    "                print(f'Validation loss : {epoch_loss}')\n",
    "                if best_val_loss is None or epoch_loss < best_val_loss:\n",
    "                    epochs_no_improve = 0\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= patience:\n",
    "                        print('Early stopping')\n",
    "                        model.load_state_dict(best_model_wts)\n",
    "                        return model\n",
    "                \n",
    "        print(f'Epoch duration : {time.time() - epoch_start}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be used to make predictions with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "8dcdb4c6-53a0-4861-b763-cfc134c8443a",
    "_uuid": "4096982a-f962-4b2b-9244-ab89804d61c7"
   },
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(normalize_rssi(X), dtype=torch.float32).cuda()\n",
    "        outputs = model(inputs).squeeze(-1)\n",
    "        return outputs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input normalization gives us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "498905ab-9e75-4b2f-a69d-606ec56ed845",
    "_uuid": "59c29413-27be-49d6-970c-0a3038e45681"
   },
   "outputs": [],
   "source": [
    "def normalize_rssi(rssi):\n",
    "    return (110 + rssi)/110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add white Gaussian noise to input data to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "dc3e060e-f5a0-4508-89bc-90a8f0101f08",
    "_uuid": "606be14d-35fd-40a5-ad37-8e792d89f1d6"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()).cuda() * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "1570109c-38f5-4a65-a2cf-1f2d788b6564",
    "_uuid": "b4ca5c31-f181-4227-8f37-65f6aafccd56"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(normalize_rssi(train_X), dtype=torch.float32), torch.tensor(train_building)), batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(normalize_rssi(val_X), dtype=torch.float32), torch.tensor(val_building)), batch_size=128, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the building classifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "426955f9-ce4b-486b-8ffc-094c7a87885b",
    "_uuid": "614f7ea4-1ff2-4fc5-bad5-934fb2a90117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss : 0.12450788999960674\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.019261302578824517\n",
      "Epoch duration : 1.6629669666290283\n",
      "Epoch 2\n",
      "Training loss : 0.05315937665934727\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.013369251083692206\n",
      "Epoch duration : 1.550142765045166\n",
      "Epoch 3\n",
      "Training loss : 0.048001321214364796\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.010435926489555962\n",
      "Epoch duration : 1.957895278930664\n",
      "Epoch 4\n",
      "Training loss : 0.04567119433749013\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.010584441272358698\n",
      "Epoch duration : 1.6323180198669434\n",
      "Epoch 5\n",
      "Training loss : 0.03816487384218026\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.010306759979250594\n",
      "Epoch duration : 1.5441865921020508\n",
      "Epoch 6\n",
      "Training loss : 0.039542515875726214\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.01148200830870979\n",
      "Epoch duration : 1.5804731845855713\n",
      "Epoch 7\n",
      "Training loss : 0.03701384346629304\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.013008972057244083\n",
      "Epoch duration : 1.798020362854004\n",
      "Epoch 8\n",
      "Training loss : 0.03196569544460049\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.011928440336572855\n",
      "Epoch duration : 1.5367212295532227\n",
      "Epoch 9\n",
      "Training loss : 0.03148130347906816\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.013061772889640826\n",
      "Epoch duration : 1.574580192565918\n",
      "Epoch 10\n",
      "Training loss : 0.030383711751754874\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.01519759603209369\n",
      "Epoch duration : 1.5498497486114502\n",
      "Epoch 11\n",
      "Training loss : 0.024234853223026768\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.015213245188052123\n",
      "Epoch duration : 1.5556659698486328\n",
      "Epoch 12\n",
      "Training loss : 0.024251256721109393\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.015409098887068788\n",
      "Epoch duration : 1.5522971153259277\n",
      "Epoch 13\n",
      "Training loss : 0.02243422526710977\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.014687958618143218\n",
      "Epoch duration : 1.5511231422424316\n",
      "Epoch 14\n",
      "Training loss : 0.023197652291264234\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.013944091320702094\n",
      "Epoch duration : 1.799037218093872\n",
      "Epoch 15\n",
      "Training loss : 0.02043642138880564\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.015341127393720158\n",
      "Epoch duration : 1.540665864944458\n",
      "Epoch 16\n",
      "Training loss : 0.019754386770092253\n",
      "Learning rate : 0.001\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation loss : 0.016252024308896044\n",
      "Epoch duration : 1.5340683460235596\n",
      "Epoch 17\n",
      "Training loss : 0.019994691612929622\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.015786411990762538\n",
      "Epoch duration : 1.5484719276428223\n",
      "Epoch 18\n",
      "Training loss : 0.01955375526662598\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.015598184448556233\n",
      "Epoch duration : 1.5402066707611084\n",
      "Epoch 19\n",
      "Training loss : 0.018304479224190904\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.01717207752763523\n",
      "Epoch duration : 1.5494816303253174\n",
      "Epoch 20\n",
      "Training loss : 0.017617990494690594\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.016306534274650297\n",
      "Epoch duration : 1.573852777481079\n",
      "Epoch 21\n",
      "Training loss : 0.018613650073577476\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.016282002829885803\n",
      "Epoch duration : 1.7842438220977783\n",
      "Epoch 22\n",
      "Training loss : 0.018115337896425793\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.01651194008627499\n",
      "Epoch duration : 1.557426929473877\n",
      "Epoch 23\n",
      "Training loss : 0.01788801673943616\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.015739593924348916\n",
      "Epoch duration : 1.8815758228302002\n",
      "Epoch 24\n",
      "Training loss : 0.01693216370933545\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.015524868661112337\n",
      "Epoch duration : 1.6925766468048096\n",
      "Epoch 25\n",
      "Training loss : 0.017529900181031507\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.016077967856664842\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "building_clf = MLPClf(num_outputs=3).cuda()\n",
    "\n",
    "optimizer = optim.Adam(building_clf.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "building_clf = train(building_clf, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=200, patience=20, transform=AddGaussianNoise(0.0, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the building classifier on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "76a59355-8d1c-43bb-8238-3004cc037eca",
    "_uuid": "23bf60f1-6420-4d34-ba56-46c47d12e825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation building accuracy :  0.9983682983682983\n"
     ]
    }
   ],
   "source": [
    "pred_building = np.argmax(predict(building_clf, val_X), axis=1)\n",
    "\n",
    "print('Validation building accuracy : ', accuracy(pred_building, val_building))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the floor classifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss : 0.6665984622346677\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.3451508802964407\n",
      "Epoch duration : 1.493243932723999\n",
      "Epoch 2\n",
      "Training loss : 0.4123702911826653\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.2820905656279886\n",
      "Epoch duration : 1.5758042335510254\n",
      "Epoch 3\n",
      "Training loss : 0.3256432318348226\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.2584413898122661\n",
      "Epoch duration : 1.709512710571289\n",
      "Epoch 4\n",
      "Training loss : 0.2719575717924087\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.2486893957042519\n",
      "Epoch duration : 1.4719984531402588\n",
      "Epoch 5\n",
      "Training loss : 0.22938896122017527\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.22199852622168906\n",
      "Epoch duration : 1.5245661735534668\n",
      "Epoch 6\n",
      "Training loss : 0.2082682261622049\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.21510299536235192\n",
      "Epoch duration : 1.4582386016845703\n",
      "Epoch 7\n",
      "Training loss : 0.18778171814311811\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.23434145556872382\n",
      "Epoch duration : 1.4348077774047852\n",
      "Epoch 8\n",
      "Training loss : 0.17716531110246006\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.23825303251471588\n",
      "Epoch duration : 1.4854896068572998\n",
      "Epoch 9\n",
      "Training loss : 0.17393770874515782\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.23590653104817166\n",
      "Epoch duration : 1.504845380783081\n",
      "Epoch 10\n",
      "Training loss : 0.16348121811946234\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.22977552502689993\n",
      "Epoch duration : 1.9037580490112305\n",
      "Epoch 11\n",
      "Training loss : 0.15251903980970383\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.23368492969037855\n",
      "Epoch duration : 1.563169240951538\n",
      "Epoch 12\n",
      "Training loss : 0.15058361799494038\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.23133500221678438\n",
      "Epoch duration : 1.5625464916229248\n",
      "Epoch 13\n",
      "Training loss : 0.14565801820377025\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.22345376861117341\n",
      "Epoch duration : 1.5459413528442383\n",
      "Epoch 14\n",
      "Training loss : 0.13945323307582033\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.2506409876377267\n",
      "Epoch duration : 1.4846289157867432\n",
      "Epoch 15\n",
      "Training loss : 0.13963997594224728\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.23262091638410792\n",
      "Epoch duration : 1.4876492023468018\n",
      "Epoch 16\n",
      "Training loss : 0.1347759127374587\n",
      "Learning rate : 0.001\n",
      "Validation loss : 0.22643272666370168\n",
      "Epoch duration : 1.505425214767456\n",
      "Epoch 17\n",
      "Training loss : 0.12806364536527695\n",
      "Learning rate : 0.001\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Validation loss : 0.23152501557898872\n",
      "Epoch duration : 1.7777585983276367\n",
      "Epoch 18\n",
      "Training loss : 0.1267156301414579\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.24017804000964937\n",
      "Epoch duration : 1.4690158367156982\n",
      "Epoch 19\n",
      "Training loss : 0.12105818901483606\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.23720577277024002\n",
      "Epoch duration : 1.8528542518615723\n",
      "Epoch 20\n",
      "Training loss : 0.11824149580869248\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.2397492451264578\n",
      "Epoch duration : 1.6493072509765625\n",
      "Epoch 21\n",
      "Training loss : 0.1159070190496561\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.24513446975170688\n",
      "Epoch duration : 1.514158010482788\n",
      "Epoch 22\n",
      "Training loss : 0.10914391685065215\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.23001039581482902\n",
      "Epoch duration : 1.5076994895935059\n",
      "Epoch 23\n",
      "Training loss : 0.11338547165922033\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.24251594284878059\n",
      "Epoch duration : 1.7099883556365967\n",
      "Epoch 24\n",
      "Training loss : 0.10939246915825983\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.2508636875735486\n",
      "Epoch duration : 1.6683664321899414\n",
      "Epoch 25\n",
      "Training loss : 0.10708230100874978\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.24087733389152324\n",
      "Epoch duration : 1.5093741416931152\n",
      "Epoch 26\n",
      "Training loss : 0.10477184358893371\n",
      "Learning rate : 0.0001\n",
      "Validation loss : 0.2334506685626419\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(normalize_rssi(train_X), dtype=torch.float32), torch.tensor(train_floor)), batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(normalize_rssi(val_X), dtype=torch.float32), torch.tensor(val_floor)), batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "floor_clf = MLPClf(num_outputs=5).cuda()\n",
    "\n",
    "optimizer = optim.Adam(floor_clf.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "floor_clf = train(floor_clf, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=200, patience=20, transform=AddGaussianNoise(0.0, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutation of the floor classifier on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation floor accuracy :  0.9282051282051282\n"
     ]
    }
   ],
   "source": [
    "pred_floor = np.argmax(predict(floor_clf, val_X), axis=1)\n",
    "\n",
    "print('Validation floor accuracy : ', accuracy(pred_floor, val_floor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate our classifiers on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test building accuracy :  0.9990999099909991\n"
     ]
    }
   ],
   "source": [
    "pred_building = np.argmax(predict(building_clf, test_X), axis=1)\n",
    "\n",
    "print('Test building accuracy : ', accuracy(pred_building, test_building))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "acf8394f-b642-4a92-94f3-593ca1d0ba6f",
    "_uuid": "5b97fc2c-16b5-46b7-b64f-adcdcb59039b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test floor accuracy :  0.9297929792979298\n"
     ]
    }
   ],
   "source": [
    "pred_floor = np.argmax(predict(floor_clf, test_X), axis=1)\n",
    "\n",
    "print('Test floor accuracy : ', accuracy(pred_floor, test_floor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
